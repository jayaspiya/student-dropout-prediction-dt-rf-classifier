{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/predict_students_dropout_and_academic_success.csv',delimiter=';')\n",
    "df.columns = df.columns.str.strip().str.lower()\\\n",
    "    .str.replace(\" \",\"_\").str.replace(\"(\",\"\").str.replace(\")\",\"\").str.replace(\"/\",\"\").str.replace(\"'\",\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Data QA\n",
    "\n",
    "Ensuring the Integrity of Your Data\n",
    "\n",
    "Key Areas to Address:\n",
    "- Table Structure\n",
    "- Variable Types\n",
    "- Null Values\n",
    "- Range Calculations\n",
    "- Count Calculations\n",
    "\n",
    "`info()` gives general information about the DataFrame.\n",
    "\n",
    "```python\n",
    "df.info(verbose=False)\n",
    "```\n",
    "\n",
    "**Verbose Information**\n",
    "- number of records: `4424`\n",
    "- number of columns: `37`\n",
    "- column data types: `float64(7), int64(29), object(1)`\n",
    "- non-null values in each columns: `4424` (Same for all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe Stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In the process of data preprocessing, a comprehensive evaluation of the dataset was conducted. During this examination, it was determined that the dataset contained no null values, and all categorical variables had been suitably encoded. An analysis of the imbalance ratio between the dropout and graduate classes revealed a ratio of 0.64. Although this ratio does not reach the threshold for being considered extreme, it does indicate a subtle imbalance within the dataset. Consequently, we have opted to address this by proceeding with oversampling the dataset using the SMOTE technique.="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance Ratio\n",
    "\n",
    "The dataset is imbalance even though the difference is not extreme. The \"graduate\" class has more instances than the \"dropout\" class. \n",
    "\n",
    "- Majority Class: `graduate`\n",
    "- Minority Class: `dropout`\n",
    "\n",
    "Imbalance Ratio: 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax=sns.countplot(x= df['target'], palette =  \"Blues\")\n",
    "for label in ax.containers:\n",
    "    ax.bar_label(label)\n",
    "plt.title(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = df.groupby('target').size()\n",
    "display(target_distribution.reset_index())\n",
    "dropout_count = target_distribution['Dropout']\n",
    "graduate_count = target_distribution['Graduate']\n",
    "imbalance_ratio = dropout_count/graduate_count\n",
    "print(\"Imbalance Ratio: {:.2f}\".format(imbalance_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Features & Target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = df[df['target']!='Enrolled']\n",
    "X = working_df.drop(columns=['target'])\n",
    "y = working_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Oversample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution before oversampling:\")\n",
    "print(Counter(y))\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the class distribution after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = np.where(y_resampled == 'Dropout', 1, 0)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_resampled,y_binary,stratify=y_resampled)\n",
    "\n",
    "print(\"Train Dataset:\",x_train.shape)\n",
    "print(\"Test Dataset:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Most of the column values are coded. You can find their values at the end of the [dataset source](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success). From the page, the reference are scraped and saved as `code_reference.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('./dataset/code_reference.csv',delimiter=';')\n",
    "print(\"reference table shape:\", reference_df.shape)\n",
    "reference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.copy()\n",
    "corr_df['target'] = corr_df['target'].apply(lambda x: 1 if x == 'Dropout' else 0)\n",
    "corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(corr_df.corr(),cmap='Blues',square = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = corr_df.iloc[:, :8] \n",
    "correlation_matrix = selected_cols.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', vmin=-1, vmax=1)\n",
    "plt.title('Corr with First 8 Cols Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = corr_df.iloc[:, 8:12]  \n",
    "correlation_matrix = selected_cols.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', vmin=-1, vmax=1)\n",
    "plt.title('Parent Occupation & Qualification Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = corr_df.iloc[:, 12:21]  \n",
    "correlation_matrix = selected_cols.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', vmin=-1, vmax=1)\n",
    "plt.title('Corr with Unrelated Middle Cols Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = corr_df.iloc[:, 21:33]  \n",
    "correlation_matrix = selected_cols.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', vmin=-1, vmax=1)\n",
    "plt.title('Curricular Units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with one row and two columns\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# Plot data on each subplot\n",
    "axes[0].set_title('1st semester Curricular Units')\n",
    "\n",
    "selected_cols_1 = corr_df.iloc[:, 21:27] \n",
    "selected_cols_1.columns = [\"credited\",\"enrolled\",\"evaluations\",\"approved\",\"grade\",\"no_evaluations\"]\n",
    "correlation_matrix_1 = selected_cols_1.corr()\n",
    "sns.heatmap(correlation_matrix_1, annot=True, cmap='Blues', vmin=-1, vmax=1,ax=axes[0])\n",
    "\n",
    "\n",
    "axes[1].set_title('2nd semester Curricular Units')\n",
    "selected_cols_2 = corr_df.iloc[:, 27:33]  \n",
    "selected_cols_2.columns = [\"credited\",\"enrolled\",\"evaluations\",\"approved\",\"grade\",\"no_evaluations\"]\n",
    "correlation_matrix_2 = selected_cols_2.corr()\n",
    "sns.heatmap(correlation_matrix_2, annot=True, cmap='Blues', vmin=-1, vmax=1,ax=axes[1])\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "# plt.suptitle('Curricular Units', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = corr_df.iloc[:, 33:]  \n",
    "correlation_matrix = selected_cols.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', vmin=-1, vmax=1)\n",
    "plt.title('Corr with global index columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = corr_df.corr()\n",
    "top_corr_cols = correlation_matrix.unstack().sort_values(ascending=False).drop_duplicates()\n",
    "\n",
    "# Select the top 10 most correlated columns (excluding the same column correlation and reverse pairs)\n",
    "top_corr_cols = top_corr_cols[(top_corr_cols != 1) & (top_corr_cols != -1)].head(10)\n",
    "top_corr_cols.name = \"correlation\"\n",
    "display(top_corr_cols.reset_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Vs Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(working_df, reference_df[reference_df['column'] == 'marital_status'],\n",
    "                left_on='marital_status', right_on='code', how='left')\n",
    "result = merged.groupby(['value', 'target']).size().reset_index(name='count')\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "result_pivot = result.pivot(index='value', columns='target', values='count')\n",
    "result_pivot.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('marital_status')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Marital Status vs. Target Distribution')\n",
    "plt.legend(title='Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(working_df, reference_df[reference_df['column'] == 'application_mode'],\n",
    "                left_on='application_mode', right_on='code', how='left')\n",
    "result = merged.groupby(['value', 'target']).size().reset_index(name='count')\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "result_pivot = result.pivot(index='value', columns='target', values='count')\n",
    "result_pivot.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('application_mode')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Application Mode vs. Target Distribution')\n",
    "plt.legend(title='Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(working_df, reference_df[reference_df['column'] == 'educational_special_needs'],\n",
    "                left_on='educational_special_needs', right_on='code', how='left')\n",
    "result = merged.groupby(['value', 'target']).size().reset_index(name='count')\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "result_pivot = result.pivot(index='value', columns='target', values='count')\n",
    "result_pivot.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('educational_special_needs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Educational Special Needs vs. Target Distribution')\n",
    "plt.legend(title='Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(working_df, reference_df[reference_df['column'] == 'gender'],\n",
    "                left_on='gender', right_on='code', how='left')\n",
    "result = merged.groupby(['value', 'target']).size().reset_index(name='count')\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "result_pivot = result.pivot(index='value', columns='target', values='count')\n",
    "result_pivot.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('gender')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Gender vs. Target Distribution')\n",
    "plt.legend(title='Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['admission_grade'])\n",
    "plt.title('Outlier Detection for Admission Grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['age_at_enrollment'])\n",
    "plt.title('Outlier Detection for Age at Enrollment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "features = df.columns\n",
    "classes = ['Graduate','Dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = dt.predict(x_test)\n",
    "y_train_pred = dt.predict(x_train)\n",
    "test_accuracy_score =  round(accuracy_score(y_test, y_test_pred),2)\n",
    "train_accuracy_score =  round(accuracy_score(y_train, y_train_pred),2)\n",
    "\n",
    "cm_test = confusion_matrix(y_test_pred, y_test)\n",
    "cm_train = confusion_matrix(y_train_pred, y_train)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_train, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Training Dataset\\nAccuracy: ' + str(train_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_test, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Test Dataset\\nAccuracy: ' + str(test_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': [2,4,6,8,10,12],\n",
    "         'min_samples_split': [2,3,4],\n",
    "         'min_samples_leaf': [1,2]}\n",
    "\n",
    "pre_dt = DecisionTreeClassifier()\n",
    "gcv = GridSearchCV(estimator=pre_dt,param_grid=params)\n",
    "# cross-validate each combination to estimate performance\n",
    "gcv.fit(x_train,y_train)\n",
    "# Optimal Performance Hyperparameters\n",
    "best_hyperparameters = gcv.best_params_\n",
    "gcv_dt = gcv.best_estimator_\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = gcv_dt.predict(x_test)\n",
    "y_train_pred = gcv_dt.predict(x_train)\n",
    "test_accuracy_score =  round(accuracy_score(y_test, y_test_pred),2)\n",
    "train_accuracy_score =  round(accuracy_score(y_train, y_train_pred),2)\n",
    "\n",
    "cm_test = confusion_matrix(y_test_pred, y_test)\n",
    "cm_train = confusion_matrix(y_train_pred, y_train)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_train, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Training Dataset\\nAccuracy: ' + str(train_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_test, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Test Dataset\\nAccuracy: ' + str(test_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(gcv_dt,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(gcv_dt, feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dt.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "plt.scatter(ccp_alphas,node_counts)\n",
    "plt.scatter(ccp_alphas,depth)\n",
    "plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Nodes vs Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in clfs:\n",
    "    y_train_pred = c.predict(x_train)\n",
    "    y_test_pred = c.predict(x_test)\n",
    "    train_acc.append(accuracy_score(y_train_pred,y_train))\n",
    "    test_acc.append(accuracy_score(y_test_pred,y_test))\n",
    "\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,test_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,test_acc,label='test_accuracy',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ccp = DecisionTreeClassifier(random_state=0,ccp_alpha=0.005)\n",
    "dt_ccp.fit(x_train, y_train)\n",
    "y_test_pred = dt_ccp.predict(x_test)\n",
    "y_train_pred = dt_ccp.predict(x_train)\n",
    "test_accuracy_score =  round(accuracy_score(y_test, y_test_pred), 2)\n",
    "train_accuracy_score =  round(accuracy_score(y_train, y_train_pred),2)\n",
    "\n",
    "cm_test = confusion_matrix(y_test_pred, y_test)\n",
    "cm_train = confusion_matrix(y_train_pred, y_train)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_train, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Training Dataset\\nAccuracy: ' + str(train_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_test, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Test Dataset\\nAccuracy: ' + str(test_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt_ccp,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(dt_ccp, feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC & AUC Analysis\n",
    "\n",
    "- The Receiver Operating Characteristic (ROC) curve and its associated area under the curve (AUC) provide valuable insights into the performance of a binary classification model, such as a Decision Tree. \n",
    "- The AUC ranges from 0 to 1, where a higher value indicates better classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict probabilities for the positive class ('Dropout' in this case)\n",
    "y_probs = dt.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs, pos_label=1)  # Specify the positive class label\n",
    "# Calculate the area under the ROC curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# Predict probabilities for the positive class ('Dropout' in this case)\n",
    "y_probs_gcv = gcv_dt.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC curve\n",
    "fpr_gcv, tpr_gcv, thresholds_gcv = roc_curve(y_test, y_probs_gcv, pos_label=1)  # Specify the positive class label\n",
    "# Calculate the area under the ROC curve (AUC)\n",
    "roc_auc_gcv = auc(fpr_gcv, tpr_gcv)\n",
    "\n",
    "# Predict probabilities for the positive class ('Dropout' in this case)\n",
    "y_probs_cpp = dt_ccp.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC curve\n",
    "fpr_cpp, tpr_cpp, thresholds_cpp = roc_curve(y_test, y_probs_cpp, pos_label=1)  # Specify the positive class label\n",
    "# Calculate the area under the ROC curve (AUC)\n",
    "roc_auc_cpp = auc(fpr_cpp, tpr_cpp)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC UnPrun Decision Tree  (area = %0.2f)' % roc_auc)\n",
    "plt.plot(fpr_gcv, tpr_gcv, color='green', lw=2, label='AUC PrePrun Decision Tree (area = %0.2f)' % roc_auc_gcv)\n",
    "plt.plot(fpr_cpp, tpr_cpp, color='red', lw=2, label='AUC PostPrun Decision Tree (area = %0.2f)' % roc_auc_cpp)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "An AUC of normal Decision Tree indicates the model has a good ability to discriminate between the positive and negative classes. \n",
    "Pre-pruning involves stopping the tree construction process before it becomes too complex, which can help prevent overfitting.\n",
    "Post-pruning involves growing the tree to its full depth and then removing branches that do not contribute significantly to performance.\n",
    "Compare to normal Decision Tree, Pre-pruned & Post-pruned Decision Tree have a slightly higher AUC value that suggests they are better at distinguishing between positive and negative instances, potentially leading to better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ccp = dt_ccp.predict(x_test)\n",
    "y_test_pred_gcv = gcv_dt.predict(x_test)\n",
    "ccp_accuracy_score =  round(accuracy_score(y_test, y_test_pred_ccp), 2)\n",
    "gcv_accuracy_score =  round(accuracy_score(y_test, y_test_pred_gcv),2)\n",
    "\n",
    "cm_ccp = confusion_matrix(y_test_pred_ccp, y_test)\n",
    "cm_gcv = confusion_matrix(y_test_pred_gcv, y_test)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_ccp, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('PostPrunning\\nAccuracy: ' + str(ccp_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_gcv, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('PrePrunning\\nAccuracy: ' + str(gcv_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier with 100 trees\n",
    "rfclf = RandomForestClassifier(n_estimators=100, max_depth= 10)\n",
    "# Train the Random Forest classifier on the training data\n",
    "rfclf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rfclf.predict(x_test)\n",
    "y_train_pred = rfclf.predict(x_train)\n",
    "test_accuracy_score =  round(accuracy_score(y_test, y_test_pred),2)\n",
    "train_accuracy_score =  round(accuracy_score(y_train, y_train_pred),2)\n",
    "\n",
    "cm_test = confusion_matrix(y_test_pred, y_test)\n",
    "cm_train = confusion_matrix(y_train_pred, y_train)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_train, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Training Dataset\\nAccuracy: ' + str(train_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_test, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Test Dataset\\nAccuracy: ' + str(test_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Perform a grid search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rfclf, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the Optimal hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "gcv_rfclf = gcv.best_estimator_\n",
    "\n",
    "print(\"Optimal Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = gcv_rfclf.predict(x_test)\n",
    "y_train_pred = gcv_rfclf.predict(x_train)\n",
    "test_accuracy_score =  round(accuracy_score(y_test, y_test_pred),2)\n",
    "train_accuracy_score =  round(accuracy_score(y_train, y_train_pred),2)\n",
    "\n",
    "cm_test = confusion_matrix(y_test_pred, y_test)\n",
    "cm_train = confusion_matrix(y_train_pred, y_train)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_train, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Training Dataset\\nAccuracy: ' + str(train_accuracy_score))\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_test, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Test Dataset\\nAccuracy: ' + str(test_accuracy_score))\n",
    "\n",
    "plt.suptitle('Confusion Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the positive class ('Dropout' in this case)\n",
    "y_probs_rf = gcv_rfclf.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC curve\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_probs_rf, pos_label=1)  # Specify the positive class label\n",
    "# Calculate the area under the ROC curve (AUC)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC UnPrun Decision Tree  (area = %0.2f)' % roc_auc)\n",
    "plt.plot(fpr_gcv, tpr_gcv, color='green', lw=2, label='AUC PrePrun Decision Tree (area = %0.2f)' % roc_auc_gcv)\n",
    "plt.plot(fpr_cpp, tpr_cpp, color='red', lw=2, label='AUC PostPrun Decision Tree (area = %0.2f)' % roc_auc_cpp)\n",
    "plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label='AUC Random Forest (area = %0.2f)' % roc_auc_rf)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ccp = dt_ccp.predict(x_test)\n",
    "y_test_pred_gcv = gcv_dt.predict(x_test)\n",
    "y_test_pred_rf = gcv_rfclf.predict(x_test)\n",
    "ccp_accuracy_score =  round(accuracy_score(y_test, y_test_pred_ccp), 2)\n",
    "gcv_accuracy_score =  round(accuracy_score(y_test, y_test_pred_gcv),2)\n",
    "rf_accuracy_score =  round(accuracy_score(y_test, y_test_pred_rf),2)\n",
    "\n",
    "cm_ccp = confusion_matrix(y_test_pred_ccp, y_test)\n",
    "cm_gcv = confusion_matrix(y_test_pred_gcv, y_test)\n",
    "cm_rf = confusion_matrix(y_test_pred_rf, y_test)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_rf, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[2])\n",
    "axes[2].set_title('Random Forset\\nAccuracy: ' + str(rf_accuracy_score), fontsize=16)\n",
    "\n",
    "# Plot heatmap for the training set\n",
    "sns.heatmap(cm_ccp, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[1])\n",
    "axes[1].set_title('Postprunning Decision Tree\\nAccuracy: ' + str(ccp_accuracy_score), fontsize=16)\n",
    "\n",
    "# Plot heatmap for the test set\n",
    "sns.heatmap(cm_gcv, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g', ax=axes[0])\n",
    "axes[0].set_title('Preprunning Decision Tree\\nAccuracy: ' + str(gcv_accuracy_score), fontsize=16)\n",
    "\n",
    "# plt.suptitle('Confusion Matrix', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Prepruning and postpruning techniques both played a role in enhancing the decision tree model's accuracy while maintaining a good AUC score. Prepruning, in particular, had a significant impact on improving AUC.\n",
    "- Random Forest demonstrated consistent accuracy on both train and test data, and it consistently performed well in terms of AUC as well.\n",
    "- Prepruned decision tree and random forest models are the most promising options for this dataset, as they achieve higher accuracy and maintain strong AUC scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dt_ccp, open('model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
